# 数据流

[toc]

## 概率计数

### Morris 算法

考虑这样一个计数器问题：数据流中只有一种数据，也就是“按动计数器”操作。你的计数器可能随时被查询总共被按了多少次

这个问题十分平凡，一个精确算法如下：

* 维护一个数 $n$，初始时为 0
* 计数器被按动时，令 $n\gets n + 1$
* 查询时，返回 $n$

显然我们需要 $\Theta(\log n)$ 个比特来维护这个数据

但是实际上算法可以使用更少的空间。以下给出一个空间复杂度为 $\order{\log\log n}$ 的算法（Morris）

* 维护一个数 $X$，初始时为 $0$
* 计数器被按动时，以 $2^{-X}$ 的概率令 $X\gets X + 1$
* 查询时，返回 $\tilde n = 2^X - 1$

### 算法分析

>  一个好的算法需要什么呢？需要以很大的概率给出小误差
>
>  只要 $\mathbb E\qty[\tilde n] = n$ 且 ${\rm Var}\qty[\tilde n]$ 不太大，那么我们可以使用切比雪夫不等式来证明算法以很大的概率相对误差比较小。我们首先证明 $\mathbb E\qty[\tilde n] = n$

设 $X_n$ 是按下计数器 $n$ 次后 $X$ 的值

**引理：**$\mathbb E\qty[2^{X_n}] = n + 1$

>  我们尝试找出 $\mathbb E\qty[2^{X_{n + 1}}]$ 与 $\mathbb E\qty[2^{X_n}]$ 之间的关系
>
> 假如 $X_n = j$，那么我们很容易得出 $X_{n + 1}$ 的分布：有 $2^{-j}$ 的概率变成 $j + 1$，有 $1 - 2^{-j}$ 的概率不变。以此可以求出 $X_n = j$ 的时候 $X_{n + 1}$ 的期望。同时 $X_n = j$ 的时候 $X_n$ 的期望就是 $j$，我们在这个基础上进行递推

**证明：**考虑到 $X_0 = 0$ 即 $\mathbb E\qty[2^{X_0}] = 1$，而且
$$
\begin{aligned}
\mathbb E\qty[2^{X_{n + 1}}] &= \sum_{j = 0}^{\infty}\mathbb E\qty[2^{X_{n + 1}} \mid X_n = j]\mathbb P[X_n = j]\\
&= \sum_{j = 0}^{\infty} \qty(2^{j + 1}\cdot \mathbb P\qty[X_{n + 1} = j + 1 \mid X_n = j] + 2^j\cdot\mathbb P\qty[X_{n + 1} = j \mid X_n = j])\mathbb P\qty[X_n = j]\\
&= \sum_{j = 0}^\infty \qty(2^{j + 1}\cdot 2^{-j} + 2^j\cdot (1 - 2^{-j}))\mathbb P\qty[X_n = j]\\
&= \sum_{j = 0}^\infty\qty(2^j + 1)\mathbb P\qty[X_n = j]\\
&= \sum_{j = 0}^\infty 2^j\mathbb P\qty[X_n = j] + \sum_{j = 0}^{\infty}\mathbb P\qty[X_n = j]\\
&= \mathbb E[2^{X_n}] + 1
\end{aligned}
$$
很容易递推得到 $\mathbb E\qty[2^{X_n}] = n + 1$

由以上引理与 $\tilde n = 2^{X_n} - 1$ 可以得知 $\mathbb E\qty[\tilde n] = n$

> 然后我们求出 ${\rm Var}\qty[\tilde n]$，使用与前文类似的方法

观察到 ${\rm Var}\qty[\tilde n] = {\rm Var}\qty[2^{X_n}] = \mathbb E\qty[\qty(2^{X_n})^2] - \mathbb E^2\qty[2^{X_n}] = \mathbb E\qty[2^{2X_n}] - (n + 1)^2$。我们首先求出 $\mathbb E\qty[2^{2X_n}]$

考虑到 $X_0 = 0$ 即 $\mathbb E\qty[2^{2X_0}] = 1$，而且
$$
\begin{aligned}
\mathbb E\qty[2^{2X_{n + 1}}] &= \sum_{j = 0}^{\infty}\mathbb E\qty[2^{2X_{n + 1}} \mid X_n = j]\mathbb P[X_n = j]\\
&= \sum_{j = 0}^{\infty} \qty(2^{2j + 2}\cdot \mathbb P\qty[X_{n + 1} = j + 1 \mid X_n = j] + 2^{2j}\cdot\mathbb P\qty[X_{n + 1} = j \mid X_n = j])\mathbb P\qty[X_n = j]\\
&= \sum_{j = 0}^\infty \qty(2^{2j + 2}\cdot 2^{-j} + 2^{2j}\cdot (1 - 2^{-j}))\mathbb P\qty[X_n = j]\\
&= \sum_{j = 0}^\infty\qty(2^{2j} + 3\cdot 2^j)\mathbb P\qty[X_n = j]\\
&= \sum_{j = 0}^\infty 2^{2j}\mathbb P\qty[X_n = j] + 3\sum_{j = 0}^\infty2^j\mathbb P\qty[X_n = j]\\
&= \mathbb E[2^{2X_n}] + 3\mathbb E\qty[2^{X_n}]\\
&= \mathbb E\qty[2^{2X_n}] + 3(n + 1)
\end{aligned}
$$
易知
$$
\mathbb E\qty[2^{2X_n}] = \mathbb E\qty[2^{2X_0}] + \sum_{i = 0}^{n - 1}3(i + 1) = \frac 32n^2 + \frac 32n + 1
$$
所以
$$
{\rm Var}\qty[\tilde n] = {\rm Var}\qty[2^{X_n}] = \mathbb E\qty[2^{2X_n}] - \mathbb E^2\qty[2^{X_n}] = \frac 32n^2 + \frac 32 n + 1 - (n + 1)^2 = \frac{n^2}2 - \frac n2< \frac {n^2}2 = \order{n^2}
$$
这样切比雪夫不等式就可以估计出相对误差的概率
$$
\mathbb P\qty[\abs{\tilde n - n} > \epsilon n]\leq \frac{{\rm Var}\qty[\tilde n]}{\epsilon^2 n^2} < \frac 1{2\epsilon^2}= \order{\epsilon^{-2}}
$$

> 很遗憾，这个式子没啥用，代入 $\epsilon = 1$，这个式子不能表明本算法有很大的概率误差小于 100%

### Morris+ 算法

为了能给出一个有效的随机计数算法，我们对 Morris 算法稍作修改（记作 Morris+ 算法）

1. 独立运行 $s$ 次 Morris 算法，设这 $s$ 个 Morris 算法的输出分别是 $\tilde n_1, \tilde n_2, \cdots, \tilde n_s$
2. 本算法的输出是这些输出的算术平均 $\tilde n = \frac 1s\sum_{i = 1}^s\tilde n_i$

我们对这个算法进行分析。首先，因为每个子 Morris 算法输出的期望都是 $n$，所以 Morris+ 算法的输出也是 $n$，而方差缩小了 $s^2$ 倍
$$
\mathbb E\qty[\tilde n] = \mathbb E\qty[\frac 1s\sum_{i = 1}^s\tilde n_i] = \frac 1s\sum_{i = 1}^s\mathbb E\qty[\tilde n_i] = \frac 1s ns = n\\
{\rm Var}\qty[\tilde n] = {\rm Var}\qty[\frac 1s\sum_{i = 1}^n\tilde n_i] = \frac 1{s^2}\sum_{i = 1}^n{\rm Var}\qty[\tilde n_i] < \frac 1{s^2}s\frac{n^2}2 < \frac{n^2}{2s}
$$
由切比雪夫不等式
$$
\mathbb P\qty[\abs{\tilde n - n} > \epsilon n] \leq \frac{{\rm Var}\qty[\tilde n]}{\epsilon^2n^2} < \frac{\frac{n^2}{2s}}{\epsilon^2n^2} = \frac 1{2s\epsilon^2}
$$
这样只要 $\frac 1{2s\epsilon^2} \leq \delta$，即 $s\geq\frac 1{2\delta\epsilon^2}$，那么 Morris+ 算法就以超过 $1 - \delta$ 的概率相对误差不超过 $\epsilon$

### Morris++ 算法

以上算法还有改进空间。算法描述如下：

1. 以 $\delta = \frac 13$ 为参数（也就是以 $s = \frac 1{2\cdot\frac 13\epsilon^2}$ 为参数）调用 $t$ 份独立的 Morris+ 算法作为子程序，设 Morris+ 算法的输出分别是 $\tilde n_1, \tilde n_2, \cdots, \tilde n_t$
2. 输出 $\tilde n_1, \tilde n_2, \cdots, \tilde n_t$ 的中位数

> 我们将 $\abs{\tilde n - n}\leq \epsilon n$，也就是相对误差不超过 $\epsilon$ 的概率计数算法输出称为 成功的。我们以 $\delta = \frac 13$ 为参数调用了 Morris+ 算法，也就是以至多 $\frac 13$ 的概率 Morris+ 子程序输出的结果是失败的。Morris+ 算法保证了当 $t$ 非常大的时候，大数定律直观地告诉我们有大约至少 $\frac 23t$ 的 Morris+ 算法子程序是成功的
>
> 而 Morris++ 算法只要有超过 $\frac 12t$ 的 Morris+ 算法子程序是成功就能输出一个成功结果——反着考虑，如果 Morris++ 输出了一个失败的结果，也就是如果 $\abs{\tilde n - n} > \epsilon n$，那么要么 $\tilde n$ 太小了，也就是 $\tilde n - n < -\epsilon n$，要么 $\tilde n$ 太大了，也就是 $\tilde n - n > \epsilon n$。如果 $\tilde n$ 过小，因为 $\tilde n$ 是中位数，所以有 $\frac 12t$ 个 Morris+ 子程序输出比 $\tilde n$ 还小，它们全都是失败的；如果 $\tilde n$ 过大，那么有 $\frac 12t$ 个 Morris+ 子程序输出比 $\tilde n$ 还大，它们也全都是失败的。只要 $\tilde n$ 失败，那么至少有 $\frac 12t$ 个 Morris+ 子程序失败
>
> 这也就表明成功的 Morris+ 算法子程序数 $\abs{\set{i\mid \abs{\tilde n_i - n}\leq\epsilon n}}$ 大于 $\frac 12t$，那么 Morris++ 算法成功，也就是 $\abs{\set{i\mid \abs{\tilde n_i - n}\leq \epsilon n}} > \frac 12t\implies \tilde n - n \leq\epsilon n$，它的逆否命题是 $\tilde n - n > \epsilon n\implies \abs{\set{i\mid\abs{\tilde n_i - n}}}\leq\frac 12t$。这就表明了 $\mathbb P[\abs{\tilde n - n} > \epsilon]\leq\mathbb P\qty[\abs{\set{i\mid\abs{\tilde n_i - n}}}\leq\frac t2]$

对于每一个 $i\leq t$，设 $Y_i = \begin{cases}1 & \abs{\tilde n_i - n} \leq\epsilon n\\0 & \abs{\tilde n_i - n} > \epsilon n\end{cases}$

那么由 Morris+ 算法的性质知 $\mathbb P\qty[Y_i = 1]>\frac 23$，这表明
$$
\mathbb E\qty[Y_i] = 1\cdot \mathbb P[Y_i = 1] + 0\cdot\mathbb P[Y_i = 0] = \mathbb P\qty[y_i = 1]>\frac 23
$$

设 $Y = \sum_{i = 1}^tY_i$，则 $Y$ 表示有多少个 Morris+ 子程序成功了，也就是 $Y = \abs{\set{i \mid \abs{\tilde n_i - n}\leq\epsilon n}}$。那么就有
$$
\mathbb E\qty[Y] = \mathbb E\qty[\sum_{i = 1}^tY_i] = \sum_{i = 1}^t\mathbb E\qty[Y_i] >\frac 23t
$$
所以
$$
\mathbb P\qty[\abs{\tilde n - n}>\epsilon n]\leq \mathbb P\qty[Y\leq\frac t2] < \mathbb P\qty[Y -\mathbb E[Y] < -\frac t6]\leq \exp(-2\qty(\frac t6)^2\frac 1t)
$$
最后一个不等式成立是 Chernoff Bound 得出的

这样，只要 $t\geq 18\ln\frac 1\delta$，Morris++ 算法就能成功

### 空间复杂度

当相对误差超过 $\epsilon$ 的概率不足 $\delta$ 的时候，Morris+ 算法调用了 $s = \Theta\qty(\frac 1{\delta\epsilon^2})$ 次 Morris 算法，而 Morris++ 算法调用了 $st = \Theta\qty(\frac 1\epsilon^2\ln\frac 1\delta)$ 次 Morris 算法。我们接下来证明：如果调用了 $s^*$ 次 Morris 算法，那么以至少 $\delta^*$ 的概率，在这 $n$ 次计数过程中每一个数都不超过 $\log\qty(\frac{s^*n}{\delta^*})$

假设某个 Morris 算法在过程中达到了 $X = \log(\frac{s^*n}{\delta^*})$，那么它再增加一次的概率是 $\frac 1{2^X}\leq\frac{\delta^*}{s^*n}$。因为 $X$ 总共只有 $n$ 个机会增加，所以 $X$ 在算法结束的时候有至多 $\frac n{2^X}\leq\frac{\delta^*}{s^*}$ 的概率增加。总共有 $s^*$ 个 Morris 算法，至多有 $s^*$ 个数达到了 $\log\qty(\frac{s^*n}{\delta^*})$ 随时准备突破。所以在算法结束的时候有至多 $\delta^*$ 的概率某个 Morris 算法的 $X$ 超过 $\log\qty(\frac{s^*n}{\delta^*})$ 了。这就表明有至少 $1 - \delta^*$ 的概率所有到达过临界值 $\log\qty(\frac{s^*n}{\delta^*})$ 的 $X$ 都不会再增长了，也就表明有至少 $1 - \delta^*$ 的概率所有 Morris 算法中的 $X$ 都不超过 $\log(\frac{s^*n}{\delta^*})$

这就表明 Morris++ 算法以至少 $1 - \delta^*$ 的概率空间复杂度为
$$
\order{st\log\log(\frac{stn}{\delta^*})} = \order{\qty(\frac 1\epsilon)^2\ln\frac 1\delta\log\log\frac {n\log\frac1\delta}{\epsilon^2\delta^*}}
$$

> 以上分析比较粗糙。一些古老的工作将这个复杂度改进到了 $\order{\log\frac 1\epsilon + \log\log n + \log\frac 1\delta}$，一些较新的工作将这个复杂度改进到了 $\Theta\qty(\log\frac 1\epsilon + \log\log n + \log\log\frac 1\delta)$

## 蓄水池抽样

你面前有一个数据流，数据在不停地流过——你只能看到每个数据一次并且不能将它们全部存储下来。你在任意时刻都可能被要求：“将刚刚你看到的所有数中均匀随机抽取一个给我。”

假设你看到的数依次为 $\set{a_i}_{i = 1, 2, \cdots, \infty}$。类似于 `std::shuffle` 的思想，你可以实现如下地算法：

* 维护变量 $s$，初始时值未定义
* 当看到数据 $a_m$ 的时候，掷骰子，以 $\frac 1m$ 的概率令 $s\gets a_m$，以 $1 - \frac 1m$ 的概率保持 $s$ 不变
* 查询时，直接输出 $s$

这个算法的正确性比较显然。当数据流中流过 $m$ 个数的时候，如果 $s = a_i$，那么第 $i$ 次掷骰子掷得了 $\frac 1i$ 将 $a_i$ 保留下来，而在第 $i + 1, i + 2, \cdots m$ 的时候掷得了 $1 - \frac 1{i + 1}, 1 - \frac 1{i + 2}, \cdots, 1 - \frac 1m$ 而没有将 $a_i$ 扔掉。这些事件都是随机的，所以
$$
\mathbb P\qty[s_m = a_i] = \frac 1i\cdot \frac i{i + 1}\cdot \frac{i + 1}{i + 2}\cdots\frac {m - 1}m = \frac 1m
$$
对于任意的 $i = 1, 2, \cdots, m$ 均成立，也就是说 $a_1, a_2, \cdots, a_m$ 能等概率地在第 $m$ 次出现

分析一下空间复杂度。我们需要存储被抽样的数与当前经过了多少数。假设所有数都不超过 $n$，那么我们的空间复杂度就是 $\order{\log n + \log m}$

## 估计不同元素个数

你需要在任意时刻估计当前数据流中不同元素的个数，并且用尽可能少的事件。设数据流是 $\set{a_i}_{i = 1, 2, \cdots}$ 而且所有数都是整数，你需要在逐一读取这些数据的时候随时准备好回答 $\abs{\set{a_1, a_2, \cdots, a_m}}$

设这个数据流中数的上界是 $n$，你在读取完前 $m$ 个数的时候被要求作答。如果需要求出精确解，有两种方法：

1. 维护一个长度为 $n$ 的数组 $v\in\R^n$ 与计数器 $X$。初始时 $v = \vec 0$。读到一个数 $a_i$ 的时候，如果 $v_{a_i} = 0$，那么就令计数器 $X\gets X + 1$，同时令 $v_{a_i}\gets 1$；如果 $v_{a_i} = 1$，表明 $a_i$ 已经出现过了，就什么都不做。这种方法需要 $n$ 个比特。
2. 直接将读过的数存下来，并维护一个计数器 $X$。当新读到一个数的时候，和之前读过的所有数比较，如果与读过的所有书都不同就令计数器加一。这种方法需要存储 $\order{m\log n}$ 个比特

我们想要一个空间复杂度是 $\order{\log nm}$ 的近似算法，也就是需要对任意的参数 $\epsilon, \delta\in(0, 1)$，求出一个 $\tilde t$ 满足 $\mathbb P\qty[\abs{t - \tilde t} > \epsilon t] < \delta$，其中 $t$ 是数据流中不同元素的个数

### 理想 FM 算法

理想 FM 算法描述如下：

1. 预处理出一个随机函数 $h: \set{1, 2, \cdots, n}\mapsto [0, 1]$
2. 维护一个 *计数器* $X$，记录当前看到所有数据被映射到的最小值，也就是 $X = \min_{i\in{\rm stream}}h(i)$
3. 输出 $\tilde t = \frac 1X - 1$

在分析这个算法之前，我们先来证明这样一个概率论中常用的性质：若 $X$ 是非负随机变量，则
$$
\mathbb E[X] = \int_0^\infty\mathbb P[X\geq\lambda]\dd{\lambda}
$$
设 $X$ 的概率密度函数是 $f(x)$，那么
$$
\begin{aligned}
\mathbb E[X] &= \int_0^{\infty}xf(x)\dd{x}\\
&= \int_0^\infty\qty(\int_0^x\dd{\lambda})f(x)\dd{x}\\
&= \iint_{0\leq\lambda\leq x\leq\infty}f(x)\dd{x}\dd{\lambda}\\
&= \int_0^\infty\qty(\int_\lambda^{\infty}f(x)\dd{x})\dd{\lambda}\\
&= \int_0^\infty\mathbb P[X\geq\lambda]\dd{\lambda}
\end{aligned}
$$
我们先证明算法期望能输出正确的结果。算法的结果只与 $X$ 相关，所以我们先分析 $X$。先分析期望。假设 $a_1, a_2, \cdots, a_m$ 是数据流，其中有 $t$ 个不同的元素，也就是说 $\abs{\set{a_1, a_2, \cdots, a_m}} = t$
$$
\begin{aligned}
\mathbb E[X] &= \int_0^\infty \mathbb P[X\geq \lambda]\dd{\lambda}\\
&= \int_0^\infty\mathbb P\qty[h(a_1)\geq \lambda\and h(a_2)\geq \lambda\and \cdots \and h(a_m)\geq \lambda]\dd{\lambda}\\
&= \int_0^\infty\mathbb P\qty[h(a)\geq\lambda]^{\abs{\set{a_1, a_2, \cdots, a_m}}}\dd{\lambda}\\
&= \int_0^1 (1 - \lambda)^t\dd\lambda\\
&= \eval{-\frac{(1 - \lambda)^{t + 1}}{t + 1}}_0^1 = \frac 1{t + 1}
\end{aligned}
$$
然后我们分析方差，为此我们先计算 $X^2$ 的期望。同样地，有
$$
\begin{aligned}
\mathbb E\qty[X^2] &= \int_0^{\infty}\mathbb P\qty[X^2\geq\lambda]\dd{\lambda}\\
&= \int_0^{\infty}\mathbb P\qty[X\geq\sqrt\lambda]\dd{\lambda}\\
&= \int_0^{\infty}\mathbb P\qty[\bigwedge_{i = 1}^mh(a_i)\geq \sqrt\lambda]\dd{\lambda}\\
&= \int_0^{\infty}\mathbb P\qty[h(a)\geq\sqrt \lambda]^{\abs{\set{a_1, a_2, \cdots, a_m}}}\dd{\lambda}\\
&= \int_0^1\qty(1 - \sqrt\lambda)^t\dd{\lambda}\\
&= 2\int_0^1\qty(1 - \lambda)^t\lambda\dd{\lambda}\\
&= 2\qty(\int_0^1(1 - \lambda)^t\dd{\lambda} - \int_0^1(1 - \lambda)^{t + 1}\dd{\lambda})\\
&= 2\qty(\frac 1{t + 1} - \frac 1{t + 2}) = \frac 2{(t + 1)(t + 2)}
\end{aligned}
$$
所以可以算出方差 ${\rm Var}(X) = \mathbb E\qty[X^2] - \mathbb E^2[X] = \frac t{(t + 1)^2(t + 2)} < \frac 1{(t + 1)^2}$

### 理想 FM+ 算法

与 Morris+ 算法类似地，我们多跑几遍 FM 就可以让它的精度高一些。FM+ 算法要求精度 $\epsilon \leq \frac 12$ 才能很好地运行。描述如下：

1. 独立运行 $s = \frac{25}{\epsilon^2\delta}$ 个 FM 算法，设它们的 *计数器* 返回 $X_1, X_2, \cdots, X_s$
2. 输出 $\tilde t = \frac 1{\overline X} - 1$，$\overline X = \frac 1s\sum_{i = 1}^sX_i$ 是这些 FM 算法 *计数器* 的均值

> 注意，这里是调用 FM 算法的计数器 $X$，而非它们的返回值 $\tilde t$。这是因为我们上面只对 $X$ 的期望和方差做出了分析，但是我们没有分析 $\tilde t$ 的期望和方差。$X$ 的期望是 $\frac 1{t + 1}$ 不能表明 $\frac 1X$ 的期望是 $t + 1$，也就不能表明 $\tilde t = \frac 1X - 1$ 的期望是 $t$

在本节中，我们仅考虑 $t\geq 1$ 的非平凡情况

> 对于平凡情况 $t = 0$，每个计数器都等于它的初值。只要所有计数器的初值都是 $+\infty$ 就可以在 $t = 0$ 的时候输出正确解

我们证明 $\overline X$ 离 $\frac 1{t + 1}$ 足够近。根据 FM 算法的分析，我们知道 $\mathbb E\qty[X_i] = \frac 1{t + 1}$ 与 ${\rm Var}\qty[X_i] < \frac 1{(t + 1)^2}$。所以
$$
\mathbb E\qty[\overline X] = \frac 1{t + 1}\\
{\rm Var}\qty[\overline X]< \frac 1s\cdot\frac 1{(t + 1)^2}
$$
代入 $s = \frac{25}{\epsilon^2\delta}$，由切比雪夫不等式知
$$
\mathbb P\qty[\abs{\overline X - \frac 1{t + 1}} > \frac{\epsilon / 5}{t + 1}] < \frac{{\rm Var}\qty[\overline X]}{\qty(\epsilon / 5)^2} < \delta
$$
所以
$$
\mathbb P\qty[\frac{1 - \epsilon / 5}{t + 1} < \overline X < \frac{1 + \epsilon / 5}{t + 1}] > 1 - \delta\\
\mathbb P\qty[\frac{t + 1}{1 + \epsilon / 5} < \frac 1{\overline X} < \frac{t + 1}{1 - \epsilon / 5}] > 1 - \delta\\
\mathbb P\qty[\frac{t - \epsilon / 5}{1 + \epsilon / 5} < \frac 1{\overline X} - 1 < \frac{t + \epsilon / 5}{1 - \epsilon / 5}] > 1 - \delta
$$

首先注意到
$$
\frac{t - \epsilon / 5}{1 + \epsilon / 5} > (t - \epsilon / 5)(1 - \epsilon / 5) >t - \frac{t + 1}5\epsilon > t - \frac 25\epsilon t > t - \epsilon t
$$

然后回想先前规定 $\epsilon < \frac 12$，也就表明
$$
\frac{t + \epsilon / 5}{1 - \epsilon / 5} < (t + \epsilon / 5)(1 + 2\epsilon / 5)= t + \frac 25\epsilon t + \frac 15\epsilon + \frac 25\epsilon\cdot\epsilon\leq t + \frac 25\epsilon t + \frac 15\epsilon t + \frac25\epsilon t = t + \epsilon t
$$
所以
$$
\mathbb P\qty[t - \epsilon t < \frac 1{\overline X} - 1 < t + \epsilon t] \geq \mathbb P\qty[\frac{t - \epsilon / 5}{1 + \epsilon / 5} < \frac 1{\overline X} - 1 < \frac{t + \epsilon / 5}{1 - \epsilon / 5}] > 1 - \delta\\

\mathbb P\qty[\abs{\qty(\frac 1{\overline X} - 1) - t} > \epsilon t] < \delta
$$

### 理想 FM++ 算法

与 Morris++ 算法类似地，我们利用多次运行 FM+ 算法得到的中位数来提高算法的性能

1. 独立运行 $q = 18\ln\frac 1\delta$ 次成功率 $\delta_{\rm FM+} = \frac 13$ 的 FM+ 算法，设它们的估计分别为 $\tilde t_1, \tilde t_2, \cdots, \tilde t_q$
2. 输出它们的中位数 $\tilde t = {\rm median}(\tilde t_1, \tilde t_2, \cdots, \tilde t_q)$

这一算法的分析与 Morris++ 可以说是一模一样了，可以证明 $\mathbb P\qty[\abs{\tilde t - t} > \epsilon t] < \delta$

我们首先令 $Y_j \triangleq \qty[\abs{\tilde t_j - t} < \epsilon t] = \begin{cases}1 & \abs{\tilde t_j - t} < \epsilon t\\0 & {\rm otherwise}\end{cases}$，然后令 $Y \triangleq \sum_{i = 1}^q Y_i$。由我们调用 FM+ 子算法时规定失败率至多为 $\delta_{\rm FM+} = \frac 13$，所以 $\mathbb E\qty[Y]\geq\frac 23q$。代入 $q = 18\ln\frac 1\delta$，由 Chernoff Bound 知
$$
\mathbb P\qty[\abs{\tilde t - t} > \epsilon t]\leq\mathbb P\qty[Y\leq\frac q2]\leq\mathbb P\qty[\abs{Y - \mathbb E[Y} < \frac q6]\leq \exp(-2\qty(\frac 16)^2q)\leq\delta
$$

### 独立哈希函数族

如果一个 $\set{1, 2, \cdots, a}\mapsto \set{1, 2, \cdots, b}$ 的哈希函数族 $\mathcal H$ 满足：对于任意的 $j_1, j_2, \cdots, j_k\in\set{1, 2, \cdots, b}$ 与 $i_1, i_2, \cdots, i_k\in\set{1, 2, \cdots, a}$，随机挑选一个哈希函数 $h\in_R\mathcal H$，有
$$
\mathbb P\qty[h(i_1) = j_1\and h(i_2) = j_2\and\cdots\and h(i_k) = j_k] = \frac 1{b^k}
$$
那么我们称这个函数族是 $k$ 元独立的

* 如果函数族 $\mathcal H$ 包含了全体 $\set{1, 2, \cdots, a}\mapsto \set{1, 2, \cdots, b}$ 的映射，那么这个 $\mathcal H$ 就是一个 $k$ 元独立的哈希函数。但是存储 $\mathcal H$ 中的元素需要 $a\log_2 b$ 个比特

* 如果 $q = p^r$ 是质数的幂（也就是有且仅有一个质因数），且 $a = b = q$，在 $q$ 阶有限域中，全体度数不超过 $k - 1$ 的多项式组成的函数族 $\mathcal H_{\rm poly} = \set{h \mid h(x) = d_0 + d_1x + \cdots + d_{k - 1}x^{k - 1}}$ 组成一个独立哈希函数族，而存储这个哈希函数族内的元素仅需要存储这 $k$ 个系数即可，只需要 $k\log q$ 个比特

> 对于任意的质数 $p$ 与整数 $r$，阶为 $p^r$ 的域是存在且唯一的，[详细证明见这个博客](https://zhuanlan.zhihu.com/p/416128323)，需要一些近世代数知识。简单点说就是将 $x^{p^r} - x$ 的所有“根”加入域中。这里给出一个需要一些数论知识能看懂的构造方法
>
> 度数不超过 $r - 1$ 的全体模 $p$ 意义下多项式的集合 $\set{A \mid A(x) = a_0 + a_1x + \cdots + a_{r - 1}x^{r - 1}, \vec a\in\mathbb F_p^r}$ 在以下加法与乘法意义下是一个阶为 $p^r$ 的域 $\mathbb F_{p^r}$：
>
> * 加法 $+$ 是普通多项式的加法：
> $$
> (A + B)(x) = (a_0 + b_0\bmod p) + (a_1 + b_1\bmod p)x + \cdots + (a_{r - 1} + b_{r - 1}\bmod p)x^{r - 1}
> $$
>
> * 在定义乘法 $\cdot$ 的时候，需要首先指定多项式环 $\mathbb F_p[x]$ 中的一个 $r$ 次首一不可约多项式 $M$（也就是 $M$ 的次数为 $r$，首项系数为 $1$ 且不存在 $M_1, M_2$ 满足 $M(x) = M_1(x)M_2(x)$ 且 $\deg M_1, \deg M_2 \geq 1$）。当计算 $A\cdot B$ 的时候，首先计算 $\mathbb F_p[x]$ 中的多项式乘法，然后除以 $M(x)$ 并取出余数，也就是
>   $$
>   C(x) \gets A(x)B(x) = \sum_{n = 0}^{2r - 2}\qty(\sum_{i + j = n}a_ib_j\bmod p)x^n\\
>   C(x) \to Q(x)M(x) + R(x), \deg R < r, R, Q\in\mathbb F_p\\
>   A\cdot B\gets R
>   $$
> 
> 遗憾的是以上构造不能直接用于证明阶为 $p^r$ 域的存在性，这是因为对于任意的质数 $p$ 与整数 $r$，$\mathbb F_p[x]$ 中 $r$ 次首一不可约多项式的存在性是依赖于 $\mathbb F_{p^r}$ 的存在唯一性的

### $k-$Minimum Values 算法

1. 取出一个 $2$ 元独立的哈希函数 $h:\set{1, 2, \cdots, n}\mapsto \set{1, 2, \cdots, M}$，其中 $M\geq n^3$
2. 维护当前看到所有数的哈希值中，最小的 $k$ 个（可以通过用插入排序维护一个有序数组，或者优先级队列实现）。这里 $k = \left\lceil\frac{24}{\epsilon^2}\right\rceil$
3. 在查询时
   * 如果目前收集到的哈希值不足 $k$ 个，那么输出目前收集到的哈希值 $k$ 个
   * 否则，设第 $k$ 小的哈希值为 $X$，那么输出 $\frac{kM}{X}$

> 首先解释一下哈希函数的选取。我们选取前文提到的多项式哈希函数。因为这里要求 $2$ 元独立哈希函数，所以取一个线性函数就行了，有两个随机参数。前文提到的多项式函数是 $\set{1, 2, \cdots, p^r}\mapsto\set{1, 2, \cdots, p^r}$ 的——事实上有限域的阶数必须是质数的幂。所以我们选取的 $M$ 是不小于 $n^3$ 的最小质数幂（一定小于 $2n^3$，因为 $n^3, n^3 + 1, \cdots, 2n^3 - 1$ 中一定有一个 $2$ 的幂），并选取相应的随机哈希函数
>
> 在这个算法中，与先前不一样的是，我们不用最小值来估计元素个数，而是用第 $k$ 小的数来估计元素个数
>
> 在理想 FM 算法中，如果我们用第 $k$ 小的数来估计元素个数，那么当数据流中有 $t$ 个不同的数时，最小的数大约是 $\frac 1{t + 1}$，那么从直观的均匀分布取理解，第 $k$ 小的数就是 $\frac k{t + 1}$。直观地，当使用大小为 $M$ 的随机哈希函数时，第 $k$ 小的数是 $\frac{k}{t + 1}M$。这样
> $$
> X\approx \frac{k}{t + 1}M\\
> t\approx \frac{kM}X - 1\approx \frac{kM}X
> $$

**定理：**如果 $\frac 1{\sqrt n} < \epsilon < \frac 12$，那么以至少 $\frac 23$ 的概率 $\tilde t$ 的相对误差不超过 $\epsilon$，也就是
$$
\mathbb P\qty[\abs{\tilde t - t} \leq \epsilon t]\geq\frac 23
$$
这个算法需要维护 $k$ 个大小不超过 $n$ 的数与一个随机哈希函数，其中 $k = \order{\frac 1{\epsilon^2}}$。这个随机哈希函数取上一节提到的多项式函数，因为是 $2$ 元独立，所以只需要维护一个线性函数，有两个系数。每个系数的数量级是 $\order{M} = \order{n^3}$，所以每个系数需要占用 $\order{\log M} = \order{\log n}$ 个比特，总空间复杂度是 $k\order{\log n} + 2\order{\log n} = \order{\frac{\log n}{\epsilon^2}}$

> 以上算法的成功率只有 $\frac 23$。提高它的成功率可以用前文提到的方法，独立运行 $18\ln \frac 1\delta$ 次取中位数就能让成功率提高到 $1 - \delta$

**证明：**

我们首先估计 $\mathbb P\qty[\tilde t > (1 + \epsilon)t]$。首先对事件的表述进行一些转换
$$
\tilde t > (1 + \epsilon)t
\iff \frac{kM}X > (1 + \epsilon)t
\iff X < \frac{kM}{(1 + \epsilon)t}
$$
因为 $X$ 是第 $k$ 小的哈希值，所以 $X < \frac{kM}{(1 + \epsilon)t}$ 等价于在全部 $t$ 个不同的哈希值中，有至少 $k$ 个被哈希到的值小于 $\frac{kM}{(1 + \epsilon)t}$ 的值上

与前文类似地，我们设 $t$ 个随机布尔变量 $Y_1, Y_2, \cdots, Y_t$，$Y_i = 1$ 等价于第 $i$ 个数被哈希到了小于 $\frac{kM}{(1 + \epsilon)t}$ 的值上，同时设 $Y = \sum_{i = 1}^tY_i$。我们估计它的期望和方差，然后用切比雪夫不等式估计 $Y\geq k$ 的概率

首先估计期望。我们假设 $h$ 是 $2$ 元独立随机哈希函数，所以 $\mathbb E[Y_i] = \mathbb P\qty[Y_i = 1] < \frac k{(1 + \epsilon)t}$。由期望的可加性，$\mathbb E\qty[Y] = \sum_{i = 1}^t\mathbb E\qty[Y_i] < \frac {tk}{(1 + \epsilon)t} = \frac k{1 + \epsilon}$

然后估计方差。因为 ${\rm Var}\qty[Y_i] = \mathbb E\qty[Y_i^2] - \mathbb E^2\qty[Y_i] < \mathbb E\qty[Y_i^2] = \mathbb E\qty[Y_i] < \frac k{(1 + \epsilon)t}$

因为 $h$ 是 $2$ 元独立哈希函数，也就是说对于任意的 $i\neq j$，$Y_i$ 与 $Y_j$ 是独立的。所以 $\mathbb E\qty[Y_iY_j] = \mathbb E\qty[Y_i]\mathbb E\qty[Y_j]$，也就是说 ${\rm Var}[Y] = \sum_{i = 1}^t{\rm Var}[Y_i] < \frac k{1 + \epsilon}$

联系到定理中的假设 $\epsilon < \frac 12$ 与 $k \geq \frac{24}{\epsilon^2}$，我们有
$$
\mathbb P\qty[\tilde t > (1 + \epsilon)t] = \mathbb P\qty[Y\geq k] \leq\mathbb P\qty[\abs{Y - \mathbb E[Y]} > k - \frac k{1 + \epsilon}] < \frac k{1 + \epsilon}\frac{(1 + \epsilon)^2}{k^2\epsilon^2} = \frac{1 + \epsilon}{\epsilon^2k} < \frac 16
$$
然后我们估计 $\mathbb P\qty[\tilde t < (1 - \epsilon)t]$。首先也是对事件的表述做一些转换
$$
\tilde t < (1 - \epsilon)t\iff \frac{kM}X < (1 - \epsilon)t\iff X > \frac{kM}{(1 - \epsilon)t}
$$
如果 $X > \frac{kM}{(1 - \epsilon)t}$，那么哈希值小于 $\frac{kM}{(1 - \epsilon)t}$ 的数一定小于 $k$ 个

同样地，设 $Z_i$ 表示第 $i$ 个数的哈希值小于 $\frac{kM}{(1 - \epsilon)t}$，再设 $Z = \sum_{i = 1}^tZ_i$

那么 $\mathbb P\qty[Z_i = 1] = \left\lfloor\frac{k}{(1 - \epsilon)t}\right\rfloor > \frac k{(1 - \epsilon)t} - \frac 1M$。联想到 $M \geq n^3 > \frac{4t}{\epsilon k}$，我们有 $\mathbb P\qty[Z_i = 1] > \frac {(1 + \epsilon)k}t - \frac{\epsilon k}{4t}$

这样我们可以估计 $\mathbb E[Z]$ 与 ${\rm Var}[Z]$，从而估计出概率

## 频繁项

### 众数查询

**输入：**一个整数数据流 $i_1, i_2, \cdots, i_m\in\set{1, 2, \cdots, n}$

**查询：**如果当前的数据中有出现次数不少于 $\frac m2$ 次的元素，那么输出它，否则随便输出一个数

解决这个问题可以使用 Misra Gries 算法：

1. 维护一个数 $I$ 和计数器 $c$。初始时 $c \gets 0$
2. 当数据流中流入一个数 $x$ 时
   * 如果 $x = I$，那么 $c\gets c + 1$
   * 如果 $x\neq I$ 且 $c = 0$，那么 $I\gets x$ 且 $c\gets 1$
   * 如果 $x\neq I$ 且 $c\neq 0$，那么 $c\gets c - 1$
3. 查询时，输出 $I$

以上算法的正确性显然，而且只需要维护两个数，空间复杂度是 $\order{\log n + \log m}$

但是以上算法的应用场景受限，一个数据流中某数出现超过一半确实不是一个常见的情景。我们将问题放松成这样：

**输入：**一个整数数据流 $i_1, i_2, \cdots, i_m\in\set{1, 2, \cdots, n}$

**查询：**输出 $k$ 个数。如果当前的数据中有出现次数不少于 $\frac mk$ 次的元素，这些元素必须被输出；如果出现次数不少于 $\frac mk$ 次的元素，那么输出它，否则随便输出一个数

解决这个问题可以把前文提到的 Misra Gries 算法修改一下：

1. 维护 $k$ 个数 $\vec I$ 和 $k$ 个计数器计数器 $\vec c$。初始时 $\vec c \gets \vec 0$
2. 当数据流中流入一个数 $x$ 时
   * *自增：*如果 $x = I_j$，那么令其计数器自增 $1$，也就是 $c_j\gets c_j + 1$
   * *自增：*如果 $x$ 不在 $\vec I$ 中但 $c_j = 0$，那么将 $x$ 放在这个位置并将计数器设为 $1$，也就是 $I_j\gets x$, $c_j\gets 1$
   * *全部自减：*如果 $x$ 不在 $\vec I$ 中且 $\vec c$ 中所有的元素都非零，那么让所有计数器的值均减 $1$，也就是 $\vec c\gets \vec c - \vec 1$
3. 查询时，输出 $\vec I$

**命题：**任意满足 $f_i > \frac{m}{k + 1}$ 的数 $i$ 在算法结束时都在这个数组中

为了证明这个命题，我们定义一个辅助变量 $\hat f_i$：如果 $i$ 在算法结束的时候在 $\vec I$ 中，那么 $\hat f_i$ 就是它对应计数器的值；如果 $i$ 不在，那么 $\hat f_i =0$。也就是 $\hat f_i = \begin{cases}c_j & I_j = i\\0 & I_j\neq i , j = 1, 2, \cdots, k\end{cases}$

**定理：**$f_i - \frac{m}{k + 1}\leq \hat f_i\leq f_i$ 对任意的数 $i = 1, 2, \cdots, n$ 均成立

**证明：**我们将算法的描述改成如下等价形式：

1. 对每个数 $i = 1, 2, \cdots, n$ 都维护一个计数器 $\hat f_i$，初始时 $\vec{\hat f}\gets \vec 0$
2. 当数据流中流入第 $j$ 个数 $i_j$ 时
   * *自增：*如果 $\hat f_{i_j} > 0$，那么 $\hat f_{i_j}\gets \hat f_{i_j} + 1$
   * *自增：*如果 $\hat f_{i_j} = 0$ 且当前计数器 $\vec{\hat f}$ 中的正数不足 $k$ 个，那么 $\hat f_{i_j}\gets 1 = \hat f_{i_j} + 1$
   * *全部自减：*如果 $\hat f_{i_j} = 0$ 且当前计数器 $\vec{\hat f}$ 中的正数至少 $k$ 个，那么令 $\vec{\hat f}$ 中所有的正数都自减 $1$

注意到以上过程中，每出现一次 $i$，$\hat f_i$ 至多自增 $1$；在不出现 $i$ 的时候 $\hat f_i$ 一定不会增加，所以由归纳很容易地知道 $\hat f_i \leq f_i$ 对所有的 $i = 1, 2, \cdots, n$ 均成立。接下来证明 $\hat f_i\geq f_i - \frac{m}{k + 1}$，也就是证明 $f_i - \hat f_i\leq \frac{m}{k + 1}$

设 $\alpha_i = f_i - \hat f_i$。数据流初始化没有流入任何数的时候，有 $\alpha = 0$。当流入第 $j$ 个数 $i_j$ 的时候

* 如果 $i_j = i$，且当前步骤中 $\hat f_i$ 自增了 $1$，那么这对应上述的第 1 种或者第 2 种 *自增* 情况，$\alpha_i$ 不会变化
* 如果 $i_j = i$，且当前步骤中 $\hat f_i$ 没有变化，那么这对应上述的第 3 种 *全部自减* 情况，$\alpha_i$ 增加 $1$
* 如果 $i_j \neq i$，且当前步骤中 $\hat f_i$ 没有变化，那么很高兴地 $\alpha_i$ 也不会变化
* 如果 $i_j \neq i$，且当前步骤中 $\hat f_i$ 自减了 $1$，那么这一定是上述的第 3 种 *全部自减* 情况导致的，$\alpha_i$ 增加 $1$

> 如果 $i_j\neq i$ 且 $\hat f_i$ 没有变化，这不一定意味着本步骤不是 *全部自减*——可能当前 $\hat f_i = 0$，也就是 $\hat f_i$ 减不动了

设 *全部自减* 发生了 $l$ 次，从上述证明种我们可以看到每次 $\alpha_i$ 增加 $1$ 都是全部自减导致的，所以 $\alpha_i\leq l$

每一次 *自增* 会让 $\sum\hat f_j$ 增加 $1$，每次 *全部自减* 会让 $\sum\hat f_j$ 减少 $k$

*全部自减* 发生了 $l$ 次，数据流种总共流入了 $m$ 个数，所以 *自增* 发生了 $m - l$ 次，*自增* 总共让 $\sum \hat f_j$ 增加了 $m - l$，而 *全部自减* 让 $\sum \hat f_j$ 减少了 $kl$

所以算法结束的时候 $\sum \hat f_j = m - l - kl = m - (k + 1)l$

因为算法的任意时刻对于任意的 $j$ 都有 $\hat f_j\geq 0$，所以算法结束的时候 $m - (k + 1)l = \sum\hat f_j\geq 0$，也就是 $l\leq\frac{m}{(k + 1)l}$

这就表明对于任意的 $i$，均有 $f_i - \hat f_i = \alpha_i\leq l \leq\frac{m}{(k + 1)l}$。证毕

设 $k + 1$ 众数集合 ${\rm HH}_{k + 1}(S) = \set{j \mid f_j > \frac{m}{k + 1}}$，Misra Gries 算法输出的集合 $H$ 一定满足 $\abs{H} = k$ 且 $H\supseteq {\rm HH}_{k + 1}$，也就是说 $H$ 是 ${\rm HH}_{k + 1}$ 一个不太大的超集。但是我们不知道 $H$ 中到底有哪些元素不属于 ${\rm HH}_{k + 1}$。我们目前也没有设计出任何只让数据流流过一次就能求出 ${\rm HH}_{k + 1}$ 的算法

Misra Gries 算法不能处理元素删除。传统数据流确实只有插入操作，但是接下来我们会介绍这种带删除的数据流

### 闸门数据流模型

> 感谢 New Bing 帮我翻译

在经典的数据流模型中，我们会流入一列数据 $i_1, i_2, \cdots, i_m$，并维护这列数据的一些统计信息。设 $\vec X$ 满足 $x_i$ 是数据流中所有等于 $i$ 的元素出现的个数，先前的算法实际上都是维护关于 $\vec X$ 的信息

但有的时候我们希望能删除先前输入的一些数据。我们在流入 $i_j\in\set{1, 2, \cdots, n}$ 的时候，同时流入一个辅助变量 $\Delta_j\in\R$。流入 $(i_j, \Delta_j)$ 意味着令 $x_{i_j}\gets x_{i_j} + \Delta_j$

闸门数据流模型不对 $\Delta_j$ 进行任何限制，只要 $\Delta_j\in\R$ 就行。下面列出一些特殊的闸门数据流模型：

* 传统数据流模型：$\Delta_j = 1$
* 收银机模型：$\Delta_j > 0$。收银机当然不会对顾客吐出钱
* 严格闸门数据流模型：$\Delta_j$ 可正可负，需满足但是任意时刻 $\vec X\in\R_{\geq 0}^n$（也就是 $\Delta_j\geq -x_{i_j}$）
* 图流模型：$\Delta_j =\pm 1$，且任意时刻 $\vec X\in\N^n$。这时 $\set{1, 2, \cdots, n}$ 对应图中潜在边的集合，也就表明动态图问题中可能没有某条边或者关于某条边有很多重边，但是不可能某条边有负数条

### 推广的众数查询

$(k, l_1)$ 单点查询问题指：任意时刻询问任意的 $i\in\set{1, 2, \cdots, n}$，需要输出 $\tilde x_i = x_i \pm \frac 1k\norm{X}_1$

$(k, l_1)$ 众数查询问题指：任意时刻进行询问，需要输出 $L\subseteq\set{1, 2, \cdots, n}$ 满足：

* $\abs{L} = \order{k}$
* 如果 $\abs{x_i} > \frac 1k\norm{X}_1$，那么 $i\in L$

> Misra Gries 算法就是在传统数据流模型下解决以上问题的一个算法

**引理：**设 $\mathcal A$ 是一个 $(3k, l_1)$ 单点查询问题的算法，其失败概率不超过 $\frac \delta n$，且消耗空间为 $s$ 比特。那么可以构造出一种解决 $(k, l_1)$ 众数查询问题的算法 $\mathcal A'$，失败概率不超过 $\delta$，消耗 $s + \order{k\log n}$ 比特

**证明：**

> 这个算法简单点说就是：遍历所有元素的出现次数，并找出出现最多的几个。注意到单点查询在这里并不是一个确定算法（因为在数据量很多，数据范围不小的前提下，将所有数记录下来就需要 $n\log n$ 个比特，是一个很大的负担），所以这里不能只找 $k$ 个，顺手多找几个

在 $\mathcal A$ 算法的基础上我们这样构造 $\mathcal A'$：查询时，遍历 $i = 1, 2, \cdots, n$，使用 $\mathcal A$ 算法依次单点查询这 $n$ 个点的值，同时记住前 $3k$ 大的数并输出

我们证明所有出现次数不低于 $\frac 1k$ 的数都被输出了。因为一个单点查询失败的概率不超过 $\frac \delta n$，所以所有单点查询失败至少一次的概率至多为 $\delta$，这些单点查询全部成功的概率至少为 $1 - \delta$

> 设第 $i$ 个单点查询失败的事件为 $A_i$，这 $\mathbb P\qty[A_i]\leq\frac\delta n$，所以 $\mathbb P\qty[\bigcup_{i = 1}^nA_i]\leq\sum_{i = 1}^n\mathbb P\qty[A_i]\leq n\cdot \frac{\delta}n = \delta$

在这些单点查询全部成功的前提下，每个返回的 $\tilde x_i$ 均在 $\qty[x_i - \frac 1{3k}\norm{x}_1, x_i + \frac 1{3k}\norm{x}_1]$ 区间内

* 如果 $x_i > \frac 1k\norm{x}_1$，那么 $\tilde x_i > \frac 1k\norm{x}_1 - \frac 1{3k}\norm{x}_1 = \frac 2{3k}\norm{x}_1$
* 如果 $x_i \leq \frac 1{3k}\norm{x}_2$，那么 $\tilde x_i\leq\frac 1{3k}\norm{x}_1 + \frac 1{3k}\norm{x}_1 = \frac2{3k}\norm{x}_1$

以上内容表明：所有 ${\rm HH}_k$ 中的元素 $i$ 都满足 $\tilde x_i > \frac 2{3k}\norm{x}_1$，所有满足 $\tilde x_i > \frac 2{3k}\norm{x}_1$ 的元素 $i$ 都满足 $i$ 是 ${\rm HH}_{3k}$ 中的元素。${\rm HH}_{3k}$ 中的元素至多有 $3k$ 个，所以满足 $\tilde x_i >\frac 2{3k}\norm{x}_1$ 的元素个数一定不超过 $3k$ 个。我们找出前 $3k$ 大的数一定包含所有满足 $\tilde x_i > \frac 2{3k}\norm{x}_1$ 的元素，所以也一定包含 ${\rm HH}_k$ 中的元素
