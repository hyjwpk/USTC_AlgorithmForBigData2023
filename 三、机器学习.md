# 机器学习

5 月 9 号到 5 月 24 号的这 5 节课笔记等 XXY “有空”的时候补上，也欢迎同学们联系我来分享笔记。这里先放一个去年记得占位。**请注意笔记中潜在的错误和蹩脚的英语**

## The Perceptron Algorithm

### Problem

**Goal: **Finding a linear separator / half-space

Given $n$ labeled examples $x_1, \cdots, x_n$ in $d-$dimensional space. Each example has label $+1, -1$. The task is to find a weight vector $w\in\R^d$ if one exists a threshold $t$ s.t.

* $w\cdot x_i > t$ for each $x$ labeled $+1$
* $w\cdot x_i < t$ for each $x$ labeled $-1$

Solutions:

* Linear Programming algorithms
* The Perception Algorithm

### Algorithm Description

$\widehat{x_i} \triangleq (x_i, 1), \widehat w \triangleq (w, -t)$

Then our goal is to find $l_i(w\cdot x_i) > 0, \forall i\in[n]$

1. Initialize $w\gets 0$
2. While there exists $x_i$ with $x_il_i\cdot w\leq 0$, update $w\gets w + x_il_i$

### Analysis

Define the margin as the minimum distance from one vector to the hyperplane, i.e. $\min\dfrac{|w^*x_i|}{\norm{w^*}_2}$.

Scale $w^*$ so that $(w^*\cdot x_i)\cdot l_i\geq 1$. So the margin is at least $\dfrac 1{\norm{w^*}_2}$. Let $r\triangleq \max\norm{x}_2$ denote the farthest point from the origin.

**Theorem 1:** If $\exists w^* \text{ s.t. }(w^*\cdot x_i)l_i\geq 1, \forall i\in[n]$, then the perception algorithm finds a solution $w$ such that $(w\cdot x_i)l_i > 0, \forall i\in[n]$ in at most $r^2\norm{w^*}_2^2$ updates.

**Proof: **Consider $w^\intercal w^*$ and $\norm{w}^2$.

1. Each update increases $w^\intercal w^*$ by at least 1.
   $$
   (w + x_il_i)\cdot w^* = w\cdot w^* + x_il_i\cdot w^*\geq w\cdot w^* + 1
   $$

2. Each update increases $\norm{w}^2$ by at most $r^2$
   $$
   \norm{w +x_il_i}^2 = \norm{w}^2 + 2l_ix_i^Tw + \norm{x_il_i}^2\leq \norm{w}^2 + r^2
   $$

Let $m$ be the number of updates of the perception algorithm. Then $w^\intercal w^*\geq m, \norm{w}^2\leq mr^2$.

By Cauchy-Schwartz inequality, $\norm{w}\norm{w^*}\geq m, \norm{w}\leq \sqrt m\cdot r$.

So $\dfrac{m}{\norm{w^*}}\leq\norm{w}\leq\sqrt{m}\cdot r$. So $m\leq\norm{w^*}^2r^2$. 

### Kernel

**Example:**

For any point $p$ on $x^2 + y^2 = 1$, we have $l_p = 1$. for any point $p$ on $x^2 + y^2 = 4$, we have $l_p = -1$. We want to classify it.

We can define $\varphi(x_i) = (x_i, \norm{x_i})$ and apply perception algorithm on $\{\varphi(x_1), \cdots, \varphi(x_n)\}$.

Note that the perceptron algorithm outputs $w$ such that $w = \sum_{i = 1}^nc_i\varphi(x_i)$. then for each $x_i$, to determine if $x_i$ is correctly classified, compute $w^\intercal\varphi(x_i) = \sum_{j = 1}^nc_j\varphi(x_j)^\intercal\varphi(x_i)$. We don't need to know $\varphi(x_i)$ exactly:

**Lemma: **Define kernel matrix $K = (k_{ij})_{n\times n}$, then exists a function $\varphi$ such that $k_{ij} = \varphi(x_i)^\intercal\varphi(x_j)$ if and only if $K$ is semi-positive definite.

**Proof: **If $K$ is a kernel matrix then $K = B^\intercal B$ where $B = \begin{bmatrix}\varphi(x_1)\\\varphi(x_2)\\\vdots\\\varphi(x_n)\end{bmatrix}$.

If $K$ is semi-positive definite then $K$ can be represented as $B^\intercal B$ and thus we can use the $i-$th row of $B$ as $\varphi(x_i)$.

**Theorem: **Suppose $k_1$ and $k_2$ are kernel functions, then:

* $ck_1$ is a legal kernel function for all $c\geq 0$
* $k_1 + k_2$ is a legal function
* $k_1k_2$ is a legal function

## PAC Learning

### Problem Description

**Assumption:** There exists a probability distribution $D$ over the instance space $X$ such that:

* The training set $S$ consists of points drawn independently at random from $D$
* Our objective is to predict well on new points that are also drawn from $D$

**Definitions:**

* $c^*$ : target concept, a subset of $X$ corresponding to the positive class for binary classification
* $h$ : hypothesis, our estimation of $c^*$
* $err_D(h)$: true error of hypothesis $h$, defined as $\mathbb P\{X\in h\oplus c^*| X\sim D\}$
* $err_S(h):$ training error of hypothesis $h$, defined as $\dfrac{|S\cap (h\oplus c^*)|}{|S|}$
* $\mathcal H$: hypothesis class. every element in $\mathcal H$ is a hypothesis

### PAC Learning

* Let $\mathcal H$ be a hypothesis class and let $\epsilon, \delta\in \R_+$. If a training set $S$ of size $n\geq \dfrac{\ln|\mathcal H| + \ln(1/\delta)}{\epsilon}$ is drawn from distribution $D$, then:
  $$
  \mathbb P\{err_S(h) > 0, \forall err_D(h)\geq \epsilon, h\in \mathcal H\}\geq 1 - \delta
  $$
  Equivalently:
  $$
  \mathbb P\{err_D(h) < \epsilon, \forall err_S(h) = 0, h\in \mathcal H\}\geq 1 - \delta
  $$
  **Proof:** Assume hypotheses $h_1, h_2, \cdots h_m$ have $err_D(h)\geq \epsilon$. Since $|S| = n$ elements are drawn from distribution $D$, we have $\mathbb P\{err_S(h_i) = 0\} \leq (1 - \epsilon)^n$. So $\mathbb P\{err_S(h_1) = 0\vee err_S(h_2) = 0\vee \cdots\vee err_S(h_m) = 0\}\leq \sum_{i = 1}^m\mathbb P\{err_S(h_i) = 0\}\leq |\mathcal H|(1 - \epsilon)^n$. And by the fact that $1 - \epsilon\leq e^{-\epsilon}$, we have the theorem.

* Let $\mathcal H$ be a hypothesis class and let $\epsilon, \delta\in\R_+$. If a training set $S$ of size $n\geq \dfrac{\ln|\mathcal H| + \ln(2/\delta)}{2\epsilon^2}$ we have
  $$
  \mathbb P\{\left|err_S(h) - err_D(h)\right|\leq \epsilon, \forall h\in\mathcal H\}\geq 1 - \delta
  $$

  **Proof:** Let $X_j$ be the random variable that hypothesis $h$ make a mistake on $j$^th^ element of $S$. So $\mathbb E(X_j) = err_D(h)$ and $X_j$ can only be $0$ or $1$. By Hoeffding bounds, $\mathbb P\{|err_S(h) - err_D(h)|\geq \epsilon\}\leq 2e^{-2n\epsilon^2}$. By union bounds, $\mathbb P\{|err_S(h) - err_D(h)|\leq \epsilon, \forall h\in\mathcal H\}\leq 2|\mathcal H|e^{-2n\epsilon^2}$, so we have the theorem.

### Occam's Razor

"Simple explanations are better than complicated ones."

**Theorem: **Fix any description language and consider a training sample $S$ from distribution $D$, then with probability $\geq 1 - \delta$, any rule $h$ with ${\rm err}_S(h) = 0$ that can be described using $b$ bits has ${\rm err}_D(h)\leq\epsilon$ if $|S| = \cfrac 1\epsilon(b\ln 2 + \ln(1 / \delta))$

Equivalently, with probability $\geq 1 - \delta$, any rule $h$ with ${\rm err}_S(h)$ that can be described using $<b$ bits will have ${\rm err}_D(h)\leq \cfrac{b\ln 2 + \ln(1 / \delta)}{|S|}$

**Proof: **Define $\mathcal H$ to be the set of rules that can be described using $< b$ bits, so $|\mathcal H|\leq 1 + 2 + 2^2 + \cdots + 2^{b - 1} < 2^b$.

## Vapnik-Chervonenkis Dimension

### Definitions

* A set of system $(X, \mathcal H)$: a set $X$, a class of subsets of $X$.
* A set system $(X, \mathcal H)$ shatters a set $A$, if each subset of $A$ can be expressed as $a\cap h$, for some $h\in\mathcal H$
* The VC dimension of $\mathcal H$ is the size of the largest set shattered by $\mathcal H$

### Examples

* Rectangles with axis-parallel edges

  $(X, \mathcal H) : X = \R^2, \mathcal H = \{\text{all exis-parallel rectangles in }\R^2\}$

  Apparently $|\mathcal H| = +\infty$

  VC-dimension: $4$

  Obviously for 4 nodes on a plane $\R^2$ that forms a good-looking diamond(one left most, one right most, one up most, one down most), every subset of $A$ can be included in a rectangle in $\mathcal H$

  But for 5 nodes, the nodes on the outer-most rectangle cannot be represented included in any rectangle

* Intervals of reals

  $(X, \mathcal H): X = \R^2, \mathcal H = \{[a, b] | a < b\}$

  VC dimension: 2

* Halfspaces in $d-$dimensions

  $X = \R^d, \mathcal H = \{\{x\in X | w^\intercal x\geq t\} | w\neq \va 0, t\in\R\}$

  VC dimension is $d + 1$

  * $\geq d + 1$

    $A = \{\vec e_1, \cdots, \vec e_d, \vec 0\}$

    For any subset $a'$ of $A$ we can find $h\in\mathcal H$ such that $a\cap h = A'$

  * $< d + 2$

    **Radon's Theorem:** Any set $s\subseteq\R^d$ with $|S|\geq d + 2$ can be partitioned into 2 disjoint subsets $S_1, S_2$ such that the convex hull of $S_1$ and the convex hull of $S_2$ are joint(intersection is not empty set)

    **Proof:** Assume $|S| = d + 2$

    Construct a matrix $A\in\R^{d\times (d + 2)}$, one column for each point in $S$

    $B = \begin{bmatrix}A\\1, 1, \cdots, 1\end{bmatrix}$

    Since $B$ only has $d + 1$ rows, $\rank B\leq d + 1$. So $B\vec x = 0$ has non-zero solution $\vec x$

    Rearrange the columns of $B$ so that $\begin{cases} x_1, \cdots, x_s \geq 0\\x_{s + 1}, \cdots, x_{d + 2} < 0\end{cases}$. So $\sum_{i = 1}^sb_i|x_i| = \sum_{i = s + 1}^{d + 2}b_i|x_i|$. Also, by the definition of $B$, we have:

    * $\sum_{i = 1}^s|x_i|a_i = \sum_{i = s + 1}^{d + 2}|x_i|a_i$
    * $\sum_{i = 1}^s|x_i| = \sum_{i = s + 1}^{d + 2}|x_i|$

    Normalize $\vec x$ so that $\norm{x}_1 = \sum_{i = 1}^s|x_i| = 1$. By the property of convex hull(every inner point can be represented as a linear combination whose sum of coefficient is 1). Proof done.
  
* Spheres in $d-$dimensions

  $X = \R^d, \mathcal H = \{\{|x - x_0| \leq r\} | x_0\in X, r\in\R_+\}$

  * $< d + 2$

    Notice that if there exists a point set $A$ of size $d + 2$ such that for each subset $A'$ of $A$ there exists $h\in \mathcal H$ such that $A' = A\cap h$, then for each subset $A_1$, let $A_2 = A\setminus A_1$, there exists 2 spheres $B_1, B_2$ such that $A_1\subset B_1, A_2\subset B_2, B_1\cap A_2 = A_1\cap B_2 = \varnothing$.

    Notice that $B_1$ and $B_2$ may intersect, but there are no points in their intersection. It's easy to see that there exists a hyperplane perpendicular to the line joining their centers with all points in $A_1$ on one side and all other points in $A_2$ on the other, and this implies that halfplanes shatters $A$.
  
  * $\geq d + 1$
    
    Notice that obvoiusly, $d$ unit vectors and $1$ origin may work.

### PAC Learning

> 后文中 $A\triangle B$ 表示 $(A\setminus B)\cup(B\setminus A)$

先前在 PAC Learning 中，我们证明了：设 $\mathcal H$ 是一个假设类，如果 $\mathcal H$ 是有限的，那么当 $n\geq \frac 1{2\epsilon^2}(\ln \abs{\mathcal H} + \ln \frac 1\delta)$ 时以至少 $1 - \delta$ 的概率，真实误差与训练误差的差距不超过 $\epsilon$

如果 $\mathcal H$ 是无限集合，但是它的 VC 维数是有限的，那么我们可以证明：当 $n\geq \frac c{\epsilon^2}\qty({\rm VC}(X, \mathcal H)\cdot\log\frac 1\epsilon + \ln\frac 1\delta)$ 时训练误差与真实误差的差距也不超过 $\epsilon$，其中 $c$ 是某个常数

为了证明这个定理，我们首先定义 *碎裂函数* (Shatter Function) $\pi_{\mathcal H}(n)$ 表示对于任意大小为 $n$ 的集合 $A$，$A$ 与 $\mathcal H$ 中的元素相交最多能生成多少个 $A$ 的子集（也就是集族 $\set{A\cap h : h\in \mathcal H}$ 的大小在 $\abs{A} = n$ 的限制下最大是多少）

接下来给出这样的一个 *关键定理*：

**定理：**设 $(X, \mathcal H)$ 是一个集合系统，$D$ 是 $X$ 上的概率分布。设 $n$ 满足 $n\geq\frac 8\epsilon$ 且 $n\geq \frac 2\epsilon\qty(\log_2(2\pi_{\mathcal H}(2n)) + \log_2(\frac 1\delta)))$。设 $S_1$ 包含了 $n$ 个以 $D$ 的概率分布从 $X$ 中抽样的点，那么以至少  $1 - \delta$ 的概率，任意的 $\mathcal H$ 中的集合 $h$ 满足：若 $h$ 在 $D$ 中所占的概率质量超过 $\epsilon$，那么 $h\cap S_1\neq\varnothing$

**证明：**设 $A$ 表示这样一个事件：存在某个 $h\in \mathcal H$，满足 $h$ 的概率质量超过了 $\epsilon$，但是 $h$ 与 $S_1$ 无交

> $A$ 表示定理中描述事件的反命题。只要证明 $\Pr[A]\leq\delta$，那么定理就成立了

我们再以 $D$ 的概率分布从 $X$ 中抽取 $n$ 个点来组成集合 $S_2$

设 $B$ 表示这样一个事件：存在 $h\in\mathcal H$ 满足 $h\cap S_1 = \varnothing$ 且 $\abs{h\cap S_2}\geq\frac{\epsilon n}2$。我们首先尝试证明这两件事：

* $\Pr[B\mid A]\geq\frac 12$（如果 $A$ 发生了那么 $B$ 很可能发生）
* $\Pr[B]\leq\frac \delta 2$（$B$ 基本上不可能发生）

如果这两件事都成立，那么
$$
\frac\delta 2\geq \Pr[B]\geq\Pr[A\cap B] = \Pr[B\mid A]\Pr[A]\geq\frac 12\Pr[A]
$$
也就表明 $\Pr[A]\leq\delta$，表明原命题成立了

接下来我们尝试证明这两点。首先尝试证明  $\Pr[B]\leq \frac\delta 2$

因为 $S_1$ 与 $S_2$ 都是以 $D$ 的概率分布抽取的，所以它们等价于这样构造（联想作业 2 的附加题）

1. 抽取一个大小为 $2n$ 的点集 $S_3$
2. 将 $S_3$ 均匀等概率划分成两个大小相同的集合 $S_1$ 与 $S_2$

由 *碎裂函数* 的定义，$S_3$ 与 $\mathcal H$ 中集合产生交集的个数 $\abs{\set{S_3\cap h: h\in \mathcal H}}$ 一定小于等于 $\pi_{\mathcal H}(2n)$

对于任意满足概率质量不小于 $\epsilon$ 的集合 $h'\in \mathcal H$，只要我们证明事件 $\abs{S_1\cap h'} = 0$ 且 $\abs{S_2\cap h'}\geq\frac{\epsilon n}2$ 的发生概率不超过 $\frac \delta{2\pi_{\mathcal H}(2n)}$，那么由 Union Bound 知存在 $h\in \mathcal H$ 满足 $\abs{S_1\cap h} = 0$ 且 $\abs{S_2\cap h}\geq\frac{\epsilon n}2$ 的概率不超过 $\frac \delta 2$

> 毕竟 $\mathcal H$ 中的元素与 $S_3$ 的交集都只有不超过 $\pi_{\mathcal H}(2n)$ 个。虽然 $\mathcal H$ 中的元素个数可能是无穷多，但是 $\abs{S_1\cap h}$ 与 $\abs{S_2\cap h'}$ 的大小显然只与 $S_3\cap h'$ 有关，毕竟 $S_1\cap h' = S_1\cap (S_3\cap h')$

接下来我们证明对于任意 $h'\in \mathcal H$，$\Pr\qty[\abs{S_1\cap h'} = 0\and \abs{S_2\cap h'}\geq \frac{\epsilon n}2]\leq \frac{\delta}{2\pi_{\mathcal H}(2n)}$：

* 当 $\abs{S_3\cap h'} < \frac{\epsilon n}2$ 时，显然 $\abs{h'\cap S_2}\geq\frac{\epsilon n}2$ 的概率是 $0$
* 当 $\abs{S_3\cap h'}\geq\frac{\epsilon n}2$ 时，事件“ $\abs{S_1\cap h'} = 0$ 且 $\abs{S_2\cap h'}\geq\frac{\epsilon n}2$” 等价于 $S_3\cap h'$ 中的所有点在随机划分的时候一个都没落在 $S_1$ 里。第一个点没落在 $S_1$ 里的概率是 $\frac n{2n} = \frac 12$，第二个点没落在 $S_1$ 里的概率是 $\frac{n - 1}{2n - 1} <\frac 12$……这 $\abs{S_3\cap h'}$ 个点都没落在 $S_1$ 里的概率一定不超过 $\qty(\frac 12)^{\abs{S_3\cap h'}}$。因为 $\abs{h'\cap S_3}\geq\frac{\epsilon n}2$，所以事件 $\abs{S_1\cap h'} = 0\and \abs{S_2\cap h'}\geq \frac{\epsilon n}2$ 的概率一定不超过 $\qty(\frac 12)^{\frac{\epsilon n}2}$。代入 $n\geq \frac 2\epsilon\qty(\log_2(2\pi_{\mathcal H}(2n)) + \log_2(\frac 1\delta))$ 知 $\Pr\qty[\abs{S_1\cap h'} = 0\and \abs{S_2\cap h'}\geq\frac{\epsilon n}2\ \big|\  \abs{h'\cap S_3}\geq \frac{\epsilon n}2]\leq \frac\delta{2\pi_{\mathcal H}(2n)}$

在两种情况下目标事件发生的概率都不超过 $\frac{\delta}{2\pi_{\mathcal H}(2n)}$，这就表明 $\abs{S_1\cap h'} = 0\and \abs{S_2\cap h'}\geq\frac{\epsilon n}2$ 的概率确实不超过 $\frac\delta{2\pi_{\mathcal H}(2n)}$，进一步表明 $\Pr\qty[B]\leq \frac\delta 2$

接下来我们证明对于任意的集合 $h'\in\mathcal H$，若 $h'$ 的质量大于等于 $\eta$，那么 $\Pr\qty[B\mid A]\geq \frac 12$

因为 $S_1$ 里点与 $S_2$ 里点都是以 $D$ 为概率分布进行独立随机抽样的，所以 $S_1$ 中的点是否在 $h$ 中与 $S_2$ 中的点是否在 $h$ 中是独立的，也就是表明 $\Pr\qty[\abs{S_1\cap h'} = 0\and \abs{S_2\cap h'}\geq\frac{\epsilon n}2\mid \abs{S_1\cap h'} = 0] = \Pr\qty[\abs{S_2\cap h'}\geq\frac{\epsilon n}2]$。我们计算后者的概率。

设 $X_i$ 表示 $S_2$ 中第 $i$ 个点是否在 $h'$ 中这个事件的指示变量，也就是 $X_i = 1$ 这个事件等价于 $S_2$ 中第 $i$ 个点在 $h'$ 中，$X_i = 0$ 这个事件等价于 $S_2$ 中第 $i$ 个点不在 $h'$ 中。那么，$\abs{S_2\cap h'} = \sum_{i = 1}^n X_i$，且 $X_i$ 是 $[0, 1]$ 内独立的随机变量。设 $h'$ 的概率质量是 $\eta$，由假设知所以 $S_2$ 中第 $i$ 个点在 $h'$ 中的概率大于等于 $\epsilon$，也就表明 $\mathbb E[X_i] = \Pr[X_i = 1] = \eta\geq\epsilon$ 且 $\mathbb E[X_i^2] = \eta$

设 $X = \sum_{i = 1}^nX_i$，则 $\mathbb E[X] = \sum_{i = 1}^n\mathbb E[X_i] = \eta n$ 且 ${\rm Var}[X]\leq \mathbb E[X^2] = \sum_{i = 1}^n\mathbb E[X_i^2] = \eta n$

考虑到 $X \geq\frac{\epsilon n}2\implies \abs{X - \mathbb E[X]}\leq\frac{\eta n}2$，再联系到我们假设 $n \geq\frac 8\epsilon$，我们可以得知
$$
\Pr\qty[X < \frac{\epsilon n}2]\leq \Pr\qty[\abs{X - \mathbb E[X]}>\frac{\eta n}2]\leq \frac{\eta n}{\qty(\frac {\eta n}2)^2}\leq \frac 4{\eta n}\leq \frac \epsilon{2\eta}\leq \frac 12
$$
这也就表明了 $\Pr[B\mid A]\geq\frac 12$

<hr>

回到机器学习。设 $(X, \mathcal H)$ 是一个假设类，$C^*\subseteq X$ 是目标集合

设 $\mathcal H' = \set{h\triangle C^*: h\in \mathcal H}$，则一个显然的事实是 ${\rm VC}(X, \mathcal H') = {\rm VC}(X, \mathcal H)$，而且它们的 *碎裂函数* $\pi_{\mathcal H}$ 和 $\pi_{\mathcal H'}$ 是相同的

有了以上事实，我们可以证明

**定理：**

* 对于任意的假设 $\mathcal H$、概率分布 $D$ 和大小至少为 $\frac 2\epsilon \qty(\log_2(2\pi_{\mathcal H}(2n)) + \log_2 \frac 1\delta)$ 的训练集合 $S$，以至少 $1 - \delta$ 的概率 $\mathcal H$ 中训练误差为 $0$ 的样本真实误差不超过 $\epsilon$
* 对于任意的假设 $\mathcal H$、概率分布 $D$ 和大小至少为 $\frac 8{\epsilon^2}\qty(\ln(2\pi_{\mathcal H}(2n)) +\ln(\frac 1\delta))$，以至少 $1 - \delta$ 的概率 $\mathcal H$ 中任意模型的训练误差和真实误差的差距不超过 $\epsilon$

前者直接对 $\mathcal H'$ 套用 *关键定理* 即可，后者证明类似普通 PAC Learning 的相关证明，使用 Chernoff-Hoeffding Bounds

$\pi_{\mathcal H}(2n)$ 是一个比较抽象的函数，以上两个定理并不明朗。为了将定理中的训练集大小下界写成 $\order{\frac 1{\epsilon^2}({\rm VC}(X, \mathcal H)\log \frac 1\epsilon + \log \frac 1\delta)}$ 的形式，我们需要以下引理：

**引理 (Saner)：**如果 ${\rm VC}(X, \mathcal H)\leq d$，那么 $\pi_{\mathcal H}(n)\leq\binom{n}{\leq d}$，其中 $\binom{n}{\leq d}\triangleq \sum_{i = 0}^d\binom ni$

**证明：**首先注意到当 $n, d\geq 1$ 的时候 $\binom{n}{\leq d} = \binom{n - 1}{\leq d - 1} + \binom{n - 1}{\leq d}$

> $\binom{n}{\leq d}$ 表示在 $n$ 个数中选择不超过 $d$ 个数的方案数。对于所有的 $n$ 个数中选择不超过 $d$ 个数的方案，要么选了第一个数，要么不选第一个数。如果选了第一个数，那么需要在剩下的 $n - 1$ 个数中选择不超过 $d - 1$ 个数，有 $\binom{n - 1}{\leq d - 1}$ 种方案；如果没选第一个数，那么需要在剩下的 $n - 1$ 个数中选择不超过 $d$ 个数，有 $\binom{n - 1}{\leq d}$ 种方案。所以 $\binom{n}{\leq d} = \binom{n - 1}{\leq d - 1} + \binom{n - 1}{\leq d}$
>
> 以上性质与 $\binom{n}{d} = \binom{n - 1}{d - 1} + \binom{n - 1}{d}$ 非常类似。如果对后者感到陌生，可以想想杨辉三角

接下来我们尝试使用归纳来证明命题

* 当 $1\leq n\leq d$ 时：一方面，$\pi_{\mathcal H}(n) = 2^n$，因为存在一个集合 $A$ 满足 $\abs{A} = d$ 且 $A$ 可以被 $\mathcal H$ *碎裂*（回想 VC 维数的定义）；另一方面，$\binom{n}{\leq d} = 2^n$，因为 $\binom{n}{\leq d}$ 的组合意义是在 $n$ 个数中选择不超过 $d$ 个数，而 $n$ 个数中不管怎么选都只能选 $n\leq d$ 个数，所以 $\binom{n}{\leq d}$ 表示在 $n$ 个数中任意选择一些数，每个数都有选或者不选两种选择，且每个数选或不选是与其他数无关的，所以有 $2^n$ 中选择方案。这表明 $\pi_{\mathcal H}(n) = 2^n = \binom{n}{\leq d}$

* 当 $d = 0$ 时：注意到 $\abs{\mathcal H} = 1$，反证这一点。

  若 $\abs{\mathcal H}\geq 2$，从 $\abs{\mathcal H}$ 中任取两个集合 $h_1$ 与 $h_2$，那么 $h_1\triangle h_2\neq\varnothing$。任取 $a\in h_1\triangle h_2$，那么要么 $a\in h_1$ 且 $a\notin h_2$，要么 $a\in h_2$ 且 $a\notin h_1$，不失一般性，假设 $a\in h_1$ 且 $a\notin h_2$ 成立

  取 $A \triangleq \set{a}$，那么 $A\cap h_1 = A$，$A\cap h_2 = \varnothing$。这表明 $A$ 可以被 $\set{h_1, h_2}$ *碎裂*，也就表明 $A$ 可以被 $\mathcal H$ *碎裂*。这表明 $d = {\rm VC}(X, \mathcal H) \geq 1$，与 $d = 0$ 矛盾

  这表明 $\abs{\mathcal H} = 1$。对于任意大小为 $n$ 个集合 $A$，$A$ 与 $\mathcal H$ 中集合的交集只有一个，也就表明 $\pi_{\mathcal H}(n) = 1$

  另一方面，从 $n$ 个数中选择不超过 $0$ 个数只有一种方案（一个数都不选），所以 $\binom{n}{\leq d} = 1 = \pi_{\mathcal H}(n)$

以上是两种基本情况，接下来考虑归纳推理：对于 $1\leq d <n$，假设引理对 $(n - 1, d - 1)$ 与 $(n - 1, d)$ 成立，使用这两个假设来证明引理对 $(n, d)$ 成立

对于任意 ${\rm VC}(X, \mathcal H) = d$ 的集族 $\mathcal H$，设其与某集合 $A\subseteq X$ 的交集 $\mathcal H'\triangleq \set{A\cap h : h\in \mathcal H}$，则由定义知我们可以选择 $A$ 满足 $\pi_{\mathcal H}(n) = \abs{\mathcal H'}$。接下来证明 $\abs{\mathcal H'}\leq\binom{n}{\leq d}$

设 $\mathcal H_1\triangleq \set{(A\setminus\set{n})\cap h : h\in \mathcal H} = \set{h\setminus \set{n} : h\in \mathcal H'}$，$\mathcal H_2\triangleq \set{h : u\notin h, h\in \mathcal H', h\cup \set{n}\in \mathcal H'}$，其中 $n$ 是任意 $A$ 中的元素，则 $\abs{\mathcal H_1} +\abs{\mathcal H_2} = \abs{\mathcal H'}$

> 这里略去了 $\mathcal H'$ 是定义在 $A$ 上的集族，$\mathcal H_1$ 与 $\mathcal H_2$ 是定义在 $A \setminus\set{n}$ 上的
>
> $\abs{\mathcal H_1} + \abs{\mathcal H_2} = \abs{\mathcal H'}$ 的正确性可以使用组合意义来说明。对于 $A\setminus \set{n}$ 中的所有子集 $h$：
>
> * 如果 $h, h\cup\set{n}\notin \mathcal H'$，那么 $h$ 的存在为等式右边贡献了 $0$；$h\notin \mathcal H_1$ 且 $h\notin \mathcal H_2$，$h$ 为等式左边贡献了 $0$
> * 如果 $h, h\cup\set{n}\in\mathcal H'$，那么 $h$ 为等式右边贡献了 $2$；$h\in \mathcal H_1$ 且 $h\in \mathcal H_2$，$h$ 为等式左边贡献了 $2$
> * 如果 $h, h\cup\set{n}$ 里恰有一个 $\mathcal H'$ 中的元素，那么 $h$ 为等式右边贡献了 $1$；$h\in \mathcal H_1$ 且 $h\notin \mathcal H_2$，$h$ 为等式左边贡献了 $1$
>
> 所以 $A\setminus\set{n}$ 的每个子集对 $\abs{\mathcal H_1} + \abs{\mathcal H_2}$ 的贡献与对 $\abs{\mathcal H'}$ 的贡献是相等的，所以 $\abs{\mathcal H_1} + \abs{\mathcal H_2} = \abs{\mathcal H'}$

同时我们注意到 $\pi_{\mathcal H}(n) = \abs{\mathcal H'} = \abs{\mathcal H_1} + \abs{\mathcal H_2}$。进一步注意到 $\abs{\mathcal H_1} \leq \pi_{\mathcal H_1}(n - 1)$，这是因为 $\mathcal H_1$ 可以把集合 $A\setminus \set{n}$ 给 *碎裂* 成 $\abs{\mathcal H_1}$ 个集合（对于任意的 $h\in \mathcal H_1$，都有 $h\subseteq A\setminus \set{n}$，也就表明 $h\cap (A\setminus\set{n}) = h$），而且 $\abs{A\setminus\set{n}} = n - 1$；同样地 $\abs{H_2}\leq\pi_{\mathcal H_2}(n - 1)$

我们可以用 $\mathcal H$ 的 VC 维数是 $d$ 这个事实来来给出 $\mathcal H_1$ 与 $\mathcal H_2$ 的 VC 维数上界：

* $\mathcal H_1$ 的 VC 维数不超过 $d$，因为它是 $\mathcal H'$ 的子集

* $\mathcal H_2$ 的 VC 维数不超过 $d - 1$

  如果有大小为 $d$ 的集合 $A'$ 能被 $\mathcal H_2$ *碎裂*，那么：

  首先 $n\notin A'$，因为 $\mathcal H_2$ 中的所有元素都不含 $n$，若 $n\in A'$ 则 $A$ 所有含 $n$ 的子集都不能与 $\mathcal H_2$ 中的集合求交获得

  其次 $A'\cup \set{n}$ 可以被 $\mathcal H'$ *碎裂*，也就是可以被 $\mathcal H$ *碎裂*。这是因为 $A'\cup \set{n}$ 所有不含 $n$ 的子集都能被 $\mathcal H_2$ *碎裂*，而所有含 $n$ 的子集能被 $\mathcal H_2\set{n}$ *碎裂*，这里 $\mathcal H_2\set{n}$ 指 $\mathcal H_2$ 中的所有集合中加上元素 $n$。同时由 $\mathcal H_2$ 的定义，$\mathcal H_2$ 与 $\mathcal H_2\set{n}$ 的元素都在 $\mathcal H'$ 中

由归纳假设，$\pi_{\mathcal H_1}(n - 1)\leq \binom{n - 1}{\leq d}, \pi_{\mathcal H_2}(n - 1)\leq\binom{n - 1}{\leq d - 1}$。全部代回，得
$$
\pi_{\mathcal H}(n) = \abs{\mathcal H'} = \abs{\mathcal H_1} + \abs{\mathcal H_2}\leq\pi_{\mathcal H_1}(n - 1) + \pi_{\mathcal H_2}(n - 1) \leq \binom{n - 1}{\leq d} + \binom{n - 1}{\leq d - 1} = \binom{n}{\leq d}
$$
由 $\binom n{\leq d}\leq n^d + 1$ 与 Saner 引理知 $\pi_{\mathcal H} (n)\leq n^d + 1$
